{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_iUHEHYEf_Gx"
   },
   "source": [
    "# Villanova University Data Mining Competition 2019 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaLDMpLFf_Gx"
   },
   "source": [
    "*You can run this notebook in jupyter notebook!*  \n",
    "*You may need to install pandas, sci-kit learn, and XGBoost*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvJ8MFgmf_Gy"
   },
   "source": [
    "## 1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abfTcR6Gf_Gz"
   },
   "source": [
    "### Problem: Numerical & Categorical Data | Empty Cells\n",
    "In this specific problem the challenge of having a mix of numerical and categorical data was present. \n",
    "In the process of looking for the best way to process the model a variety of steps were taken in order to format the data for machine learning classifiers. In specific the letters that existed in certain columns needed to be converted to integers.\n",
    "\n",
    "### Solution: Replace String Labels | Address Empty Cells\n",
    "In my final implementation for my model I replaced every letter with the next following integer in each corresponding column.\n",
    "This was done using the following function that I created.\n",
    "\n",
    "```python \n",
    "\"\"\"\n",
    "Function passes through a pandas dataframe and can be configured to \n",
    "(drop rows with empty cells or fill empty cells with -1).\n",
    "It also replaces the corresponding letter to the next unique integer in their respective category.\n",
    "\"\"\"\n",
    "\n",
    "def cleanHH(df):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Remove rows with empty cells\n",
    "    #df.dropna(inplace=True)  \n",
    "    \n",
    "    # Replace empty cells with -1\n",
    "    df.fillna(-1,inplace=True) \n",
    "\n",
    "    # Encode letters to ints in MTGd (B,D -> 9,10) & MTGdS (B,D,R -> 9,10,11)\n",
    "    df.OWN.replace(to_replace=dict(D=3,R=4), inplace=True)\n",
    "    df.MTGd.replace(to_replace=dict(B=9,D=10), inplace=True)\n",
    "    df.MTGdS.replace(to_replace=dict(B=9,D=10,R=11), inplace=True)\n",
    "```\n",
    "\n",
    "This function was very useful as it made it possible to use machine learning models afterwards.  \n",
    "Ultimately the code to address empty cells was discarded in my final implementation due to XGBoost automatically handling these blank empty cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIpx1Pltf_Gz"
   },
   "source": [
    "## 2. Data Mining Models\n",
    "\n",
    "*For every model except XGBoost I used Sci-Kit Learn to create models*\n",
    "\n",
    "### Regression\n",
    "- Logistic Regression - **Accuracy: 88.63%**\n",
    "\n",
    "### Decision Tree\n",
    "- Decision Tree Classifier - **Accuracy: 88.59%**\n",
    "- Random Forest Classifier - **Accuracy: 91.53%**  _(After Hyperparameter tuning)_\n",
    "\n",
    "### Boosting\n",
    "- Gradient Boosting Classifier - **Accuracy: 92.04%**  _(After Hyperparameter tuning)_\n",
    "- Ada Boost Classifier -         **Accuracy: 91.51%**\n",
    "- Histogram-based Gradient Boosting Classifier **Accuracy: 91.69%**\n",
    "- XGBoost -  **Accuracy: 92.55%** _(After Hyperparameter tuning)_\n",
    "\n",
    "### Neural Networks\n",
    "- Custom Keras/TF Neural Net - **Accuracy: 88.99%**\n",
    "\n",
    "As seen above I used a variety of modeling types in my approach. Logistic and Decision Tree's were obvious choices in a binary classification problem but were not delivering the accuracy desired. I moved to boosting classifiers which were giving me better results and had to find one I was comfortable with to tune. I used a Neural Network out of curiosity to see how it would fare against a binary classification problem. I was not able to push it past 88.99% accuracy no matter how many hidden layers I added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDKdt9AUf_G0"
   },
   "source": [
    "## 3. Actual Model Used [XGBoost]\n",
    "I ultimately went with XGBoost due to it's consistent high accuracy rate compared to the other models. It also had a much higher precision rate for True Positives as well which maximizes profit. The implementation I used was the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jAwhot_f_G0"
   },
   "source": [
    "### Import, store and append data into Pandas Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "si3OX7Ykf_G1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "df1 = pd.read_excel('households_1.xlsx')\n",
    "df2 = pd.read_excel('households_2_SOLN.xlsx')\n",
    "df3 = pd.read_excel('households_3.xlsx')\n",
    "\n",
    "# Combine together\n",
    "df12 = df1.append(df2, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2pFO3mtf_G4"
   },
   "source": [
    "### Create function to convert string labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ok2ET3C0f_G5"
   },
   "outputs": [],
   "source": [
    "def cleanHH2(df): \n",
    "    #pass list through and drop columns\n",
    "    \n",
    "    \n",
    "    # Remove rows if empty\n",
    "    #df.dropna(inplace=True)   #delete rows with empty cells\n",
    "    \n",
    "    # retain rows and replace None with -1\n",
    "    #df.fillna(-1,inplace=True) #replace None w/ -1\n",
    "\n",
    "    # Encode letters to ints in MTGd (B,D -> 9,10) & MTGdS (B,D,R -> 9,10,11)\n",
    "    df.OWN.replace(to_replace=dict(D=3,R=4), inplace=True)\n",
    "    df.MTGd.replace(to_replace=dict(B=9,D=10), inplace=True)\n",
    "    df.MTGdS.replace(to_replace=dict(B=9,D=10,R=11), inplace=True)\n",
    "    \n",
    "cleanHH2(df12)\n",
    "cleanHH2(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DV-7ZrM5f_G7"
   },
   "source": [
    "### Create train/test split (50/50), shuffle and stratify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdLG8ZA2f_G8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df12.drop(columns='HiEdInCh')\n",
    "y = df12['HiEdInCh'].copy()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=True,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pT6XcAhif_G-"
   },
   "source": [
    "### Hyperparameter tuned XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ym52Lvxf_G_",
    "outputId": "b8b942fd-a41a-468f-c119-faa69b865d53",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      8889\n",
      "           1       0.68      0.58      0.62      1111\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.82      0.77      0.79     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "Accuracy:  0.9233\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "\n",
    "model=xgb.XGBClassifier(random_state=1,\n",
    "                        learning_rate=0.09,\n",
    "                        n_estimators=200, \n",
    "                        gamma=1)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)\n",
    "\n",
    "print(classification_report(y_test,model.predict(X_test)))\n",
    "print(\"Accuracy: \",model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJK5jO5of_HC"
   },
   "source": [
    "### Predict HouseHold 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YJz0tpdf_HC"
   },
   "outputs": [],
   "source": [
    "## convert your array into a dataframe\n",
    "dfm = pd.DataFrame (model.predict(df3))\n",
    "dfm = dfm.rename(columns={0 : \"HiEdInCh\"})\n",
    "\n",
    "## save to xlsx file\n",
    "\n",
    "filepath = 'Final_Predictions.xlsx'\n",
    "\n",
    "dfm.to_excel(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mPgichvf_HF"
   },
   "source": [
    "## 4. Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iO1ITC_Rf_HG"
   },
   "source": [
    "1. Use boosted models, they are the most efficient for this problem. \n",
    "2. Use GPU accelerated libraries once datasets become larger.\n",
    "    - XGBoost has support for GPU Acceleration\n",
    "3. Collect more information on peoples background more features could mean better indicators for\n",
    "   better predictions\n",
    "    - Use LinkedIn to determine if someone is college educated, use their API or scrape the web.\n",
    "    - Make a heatmap using addresses of current students and warm leads to determine if \n",
    "      certain neighborhoods are hot spots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LacW2eref_HG"
   },
   "source": [
    "## 5. Table of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W-muKPF0f_HG"
   },
   "source": [
    "| Model (Classifier):               | Accuracy (%): |\n",
    "|-----------------------------------|---------------|\n",
    "| XGBoost                           | 92.55         |\n",
    "| Gradient Boosting                 | 92.04         |\n",
    "| Histogram Based Gradient Boosting | 91.69         |\n",
    "| Random Forest                     | 91.53         |\n",
    "| Adaptive Boosting                 | 91.51         |\n",
    "| Neural Network                    | 88.99         |\n",
    "| Logistic Regression               | 88.63         |\n",
    "| Decision Tree                     | 88.59         |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Data Mining Competition Report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
